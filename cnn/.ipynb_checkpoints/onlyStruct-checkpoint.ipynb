{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from Bio import SeqIO\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pdb\n",
    "import gzip\n",
    "import time\n",
    "\n",
    "# ENcoding\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, model_from_config\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Input, merge, LSTM, Merge\n",
    "from keras.models import Model # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers import normalization\n",
    "from keras.layers.convolutional import Conv1D,Convolution2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Bidirectional, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D,Convolution1D, MaxPooling1D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import History \n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# plfold\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import islice\n",
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "from os import listdir\n",
    "from  __builtin__ import any\n",
    " \n",
    "\n",
    "def read_seq(seq_file,plfold):\n",
    "    seq_list = []\n",
    "    seq = ''\n",
    "    with open(seq_file, 'r') as fp:\n",
    "        for line in fp:\n",
    "            if line[0] == '>':\n",
    "                name = line[1:-1]\n",
    "                if len(seq):\n",
    "                    seq_array = get_RNA_seq_concolutional_array(seq, name, plfold)\n",
    "                    seq_list.append(seq_array)                    \n",
    "                seq = ''\n",
    "            else:\n",
    "                seq = seq + line[:-1]\n",
    "        if len(seq):\n",
    "            seq_array = get_RNA_seq_concolutional_array(seq, name, plfold)\n",
    "            seq_list.append(seq_array) \n",
    "    \n",
    "    return np.array(seq_list)\n",
    "\n",
    "def load_label_seq(seq_file):\n",
    "    label_list = []\n",
    "    seq = ''\n",
    "    with open(seq_file, 'r') as fp:\n",
    "        for line in fp:\n",
    "            if line[0] == '>':\n",
    "                name = line[1:-1]\n",
    "                posi_label = name.split(';')[-1]\n",
    "                label = posi_label.split(':')[-1]\n",
    "                label_list.append(int(label))\n",
    "    return np.array(label_list)\n",
    "\n",
    "def get_RNA_seq_concolutional_array(seq, key, plfold, motif_len = 10):\n",
    "    seq = seq.replace('U', 'T')\n",
    "#     alpha = 'acgu'\n",
    "    alpha = 'ACGT'\n",
    "#     print seq\n",
    "    half_len = motif_len/2\n",
    "    row = (len(seq) + half_len *2 )\n",
    "    new_array = np.zeros((row, 1))\n",
    "\n",
    "#     First and last half_len values set to 0.25\n",
    "\n",
    "    for i in range(half_len):\n",
    "        new_array[i] = np.array([0.25]*1)\n",
    "    \n",
    "    for i in range(row-half_len, row):\n",
    "        new_array[i] = np.array([0.25]*1)    \n",
    "\n",
    "        \n",
    "    for i, val in enumerate(seq):\n",
    "#         print i,val\n",
    "        i = i + half_len\n",
    "#         print i,val\n",
    "#         if val not in 'acgu':\n",
    "        if val not in 'ACGT':\n",
    "            new_array[i] = np.array([0.25]*1)\n",
    "            continue\n",
    "       \n",
    "    for name in plfold:\n",
    "        if name == key:\n",
    "            for i, val in enumerate(seq):\n",
    "#                 i = i + half_len\n",
    "                new_array[i + half_len][0] = plfold[name][i]\n",
    "    \n",
    "#         using the key, find the file in the given protein directory\n",
    "#  Extract probabilities, then take sum of squares of probabilities and put in the array\n",
    "\n",
    "    \n",
    "        \n",
    "    return new_array\n",
    "\n",
    "def load_data_file(inputfile, plfold, seq = True, onlytest = False):\n",
    "    \"\"\"\n",
    "        Load data matrices from the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = os.path.dirname(inputfile)\n",
    "    data = dict()\n",
    "    if seq: \n",
    "        tmp = []\n",
    "        tmp.append(read_seq(inputfile, plfold))\n",
    "#         seq_onehot, structure = read_structure(inputfile, path)\n",
    "#         tmp.append(seq_onehot)\n",
    "        data[\"seq\"] = tmp\n",
    "#         data[\"structure\"] = structure\n",
    "    if onlytest:\n",
    "        data[\"Y\"] = []\n",
    "    else:\n",
    "        data[\"Y\"] = load_label_seq(inputfile)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def split_training_validation(classes, validation_size, shuffle = True):\n",
    "    \"\"\"split sampels based on balnace classes\"\"\"\n",
    "    num_samples=len(classes)\n",
    "    classes=np.array(classes)\n",
    "    classes_unique=np.unique(classes)\n",
    "    num_classes=len(classes_unique)\n",
    "    indices=np.arange(num_samples)\n",
    "    #indices_folds=np.zeros([num_samples],dtype=int)\n",
    "    training_indice = []\n",
    "    training_label = []\n",
    "    validation_indice = []\n",
    "    validation_label = []\n",
    "    for cl in classes_unique:\n",
    "        indices_cl=indices[classes==cl]\n",
    "        num_samples_cl=len(indices_cl)\n",
    "\n",
    "        # split this class into k parts\n",
    "        if shuffle:\n",
    "            random.shuffle(indices_cl) # in-place shuffle\n",
    "        \n",
    "        # module and residual\n",
    "        num_samples_each_split=int(num_samples_cl*validation_size)\n",
    "        res=num_samples_cl - num_samples_each_split\n",
    "        \n",
    "        training_indice = training_indice + [val for val in indices_cl[num_samples_each_split:]]\n",
    "        training_label = training_label + [cl] * res\n",
    "        \n",
    "        validation_indice = validation_indice + [val for val in indices_cl[:num_samples_each_split]]\n",
    "        validation_label = validation_label + [cl]*num_samples_each_split\n",
    "\n",
    "    training_index = np.arange(len(training_label))\n",
    "    random.shuffle(training_index)\n",
    "    training_indice = np.array(training_indice)[training_index]\n",
    "    training_label = np.array(training_label)[training_index]\n",
    "    \n",
    "    validation_index = np.arange(len(validation_label))\n",
    "    random.shuffle(validation_index)\n",
    "    validation_indice = np.array(validation_indice)[validation_index]\n",
    "    validation_label = np.array(validation_label)[validation_index]    \n",
    "    \n",
    "            \n",
    "    return training_indice, training_label, validation_indice, validation_label    \n",
    " \n",
    "\n",
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "    return y, encoder\n",
    "\n",
    "def calculate_performace(test_num, pred_y,  labels):\n",
    "    tp =0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+ fp)\n",
    "    sensitivity = float(tp)/ (tp+fn)\n",
    "    specificity = float(tn)/(tn + fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC \n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    return (f for f in listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "\n",
    "def list_dir(path):\n",
    "    proteinList=[]\n",
    "    files = os.listdir(path)\n",
    "    for name in files:\n",
    "        proteinList.append(name)\n",
    "    return proteinList\n",
    "\n",
    "\n",
    "def accuracy(test_x, test_y, model):\n",
    "    result = model.predict(test_x)\n",
    "    predicted_class = np.argmax(result, axis=1)\n",
    "    true_class = np.argmax(test_y, axis=1)\n",
    "    num_correct = np.sum(predicted_class == true_class) \n",
    "    accuracy = float(num_correct)/result.shape[0]\n",
    "    return (accuracy * 100)\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[2]:\n",
    "\n",
    "def run_network_new(name, training, y, validation, val_y, batch_size=50, nb_epoch=30):\n",
    "    print 'configure cnn network'\n",
    "    \n",
    "    rows=111\n",
    "    cols=1\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    input_shape = Input(shape=(rows,cols))\n",
    "    tower_1 = Conv1D(16, 10, padding='valid', activation='relu')(input_shape) \n",
    "    tower_temp = Activation('relu')(tower_1)\n",
    "    pool = MaxPooling1D(pool_size=3, padding='valid')(tower_temp)\n",
    "    lstm = Bidirectional(LSTM(64))(pool)\n",
    "    lstm = Dropout(0.10)(lstm)\n",
    "#     flat = Flatten()(pool)\n",
    "#     out = Dense(200, activation='relu')(flat)\n",
    "    out = Dense(50, activation='relu')(lstm)\n",
    "    out = Dense(2, activation='sigmoid')(out)\n",
    "    model = Model(input_shape, out)\n",
    " \n",
    "\n",
    "    history = History()\n",
    "    \n",
    "    model.compile(loss=\n",
    "#                   'kullback_leibler_divergence'\n",
    "                  'categorical_crossentropy'\n",
    "                  , optimizer=RMSprop(lr=0.001),metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "#     plot_model(model, to_file='architecture/%s.png'%(name))\n",
    "    \n",
    "    #tensorboard = TensorBoard(log_dir='Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    #pdb.set_trace()\n",
    "    print 'model training'\n",
    "\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    history = model.fit(training, y, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(validation, val_y)\n",
    "              , callbacks=[history\n",
    "                           ,earlystopper\n",
    "                           #,tensorboard\n",
    "                          ])\n",
    "\n",
    "    return model, history \n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "dir_path = \"/mnt/mirror/data/mimtiy/lab/encode/data/proteins/\"\n",
    "proteins = list_dir(dir_path)\n",
    "\n",
    "train_path=\"/30000/training_sample_0/\"\n",
    "test_path =\"/30000/test_sample_0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with the code (With Structural Features)\n",
      "******************************************************\n",
      "# 7  - Protein:  17_ICLIP_HNRNPC_hg19\n",
      "******************************************************\n"
     ]
    }
   ],
   "source": [
    "print \"Starting with the code (With Structural Features)\" \n",
    "\n",
    "#for protein in proteins:\n",
    "# for i in range(len(proteins)):\n",
    "i=7\n",
    "# 22\n",
    "protein = proteins[i]\n",
    "print \"******************************************************\"\n",
    "print \"#\", i ,\" - Protein: \", protein\n",
    "print \"******************************************************\"\n",
    "\n",
    "train_file_path = dir_path + protein + train_path + 'sequences.fa'\n",
    "test_file_path = dir_path + protein + test_path + 'sequences.fa'\n",
    "\n",
    "with open('pickle/train_plfold_%s.pickle'%(protein), 'rb') as handle:\n",
    "    train_plfold = pickle.load(handle)\n",
    "with open('pickle/test_plfold_%s.pickle'%(protein), 'rb') as handle:\n",
    "    test_plfold = pickle.load(handle)\n",
    "\n",
    "training_data = load_data_file(train_file_path, train_plfold)\n",
    "test_data = load_data_file(test_file_path, test_plfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Examples:  (30000,)\n",
      "Test Examples:  (10000,)\n",
      "Train Set Dimensions:  (30000, 111, 1)\n",
      "Test Set Dimensions:  (10000, 111, 1)\n",
      "Training Set Shape:  (24000, 111, 1) (24000,)\n",
      "Validation Set Shape:  (6000, 111, 1) (6000,)\n",
      "Test Set Shape:  (10000, 111, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Saving to Label and Seq \n",
    "train_Y = training_data[\"Y\"]\n",
    "test_label = test_data[\"Y\"]\n",
    "seq_data = training_data[\"seq\"][0]\n",
    "seq_test = test_data[\"seq\"][0]\n",
    "\n",
    "print \"Training Examples: \", train_Y.shape\n",
    "print \"Test Examples: \", test_label.shape\n",
    "print \"Train Set Dimensions: \", seq_data.shape\n",
    "print \"Test Set Dimensions: \", seq_test.shape\n",
    "\n",
    "# Train Test Split\n",
    "training_indice, training_label, validation_indice, validation_label = split_training_validation(train_Y,0.2, False)\n",
    "seq_train = seq_data[training_indice]\n",
    "seq_validation = seq_data[validation_indice] \n",
    "\n",
    "print \"Training Set Shape: \", seq_train.shape, training_label.shape\n",
    "print \"Validation Set Shape: \", seq_validation.shape, validation_label.shape\n",
    "print \"Test Set Shape: \", seq_test.shape, test_label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configure cnn network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 111, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 102, 16)           176       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 102, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 34, 16)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               41472     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 48,200\n",
      "Trainable params: 48,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model training\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.4950 - acc: 0.8000 - val_loss: 0.4807 - val_acc: 0.8007\n",
      "Epoch 2/30\n",
      "24000/24000 [==============================] - 23s 962us/step - loss: 0.4831 - acc: 0.7997 - val_loss: 0.4768 - val_acc: 0.8000\n",
      "Epoch 3/30\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 0.4787 - acc: 0.8000 - val_loss: 0.4731 - val_acc: 0.8030\n",
      "Epoch 4/30\n",
      "24000/24000 [==============================] - 24s 983us/step - loss: 0.4753 - acc: 0.8002 - val_loss: 0.4767 - val_acc: 0.8000\n",
      "Epoch 5/30\n",
      "24000/24000 [==============================] - 23s 976us/step - loss: 0.4731 - acc: 0.8018 - val_loss: 0.4743 - val_acc: 0.8003\n",
      "Epoch 6/30\n",
      "24000/24000 [==============================] - 24s 996us/step - loss: 0.4724 - acc: 0.7999 - val_loss: 0.4687 - val_acc: 0.7993\n",
      "Epoch 7/30\n",
      "24000/24000 [==============================] - 24s 990us/step - loss: 0.4703 - acc: 0.8002 - val_loss: 0.4739 - val_acc: 0.8020\n",
      "Epoch 8/30\n",
      "24000/24000 [==============================] - 23s 970us/step - loss: 0.4706 - acc: 0.8014 - val_loss: 0.4801 - val_acc: 0.8018\n",
      "Epoch 9/30\n",
      "24000/24000 [==============================] - 23s 957us/step - loss: 0.4700 - acc: 0.8002 - val_loss: 0.4690 - val_acc: 0.8010\n",
      "Epoch 10/30\n",
      "24000/24000 [==============================] - 24s 989us/step - loss: 0.4691 - acc: 0.8017 - val_loss: 0.4697 - val_acc: 0.8040\n",
      "Epoch 11/30\n",
      "24000/24000 [==============================] - 24s 989us/step - loss: 0.4688 - acc: 0.8015 - val_loss: 0.4725 - val_acc: 0.7990\n",
      "Epoch 00011: early stopping\n",
      "Model took 270.46 seconds to train\n"
     ]
    }
   ],
   "source": [
    "# In[7]:\n",
    "name = 'cnn1lstm+struct'\n",
    "batch_size= 50 \n",
    "nb_epoch = 30\n",
    "seq_data = []\n",
    "\n",
    "y, encoder = preprocess_labels(training_label)\n",
    "val_y, encoder = preprocess_labels(validation_label, encoder = encoder) \n",
    "test_y, encoder = preprocess_labels(test_label)\n",
    "\n",
    "training_data.clear()\n",
    "\n",
    "start = time.time()\n",
    "model, history = run_network_new(name, seq_train, y, validation = seq_validation,val_y = val_y, batch_size=batch_size, nb_epoch = nb_epoch)\n",
    "end = time.time()\n",
    "\n",
    "print \"Model took %0.2f seconds to train\"%(end - start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Set: 0.8001\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the test set!\n",
    "print \"Accuracy of Test Set:\", model.evaluate(seq_test,test_y, verbose=0)[1]\n",
    "predictions = model.predict(seq_test, verbose=0)\n",
    " # print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print predictions[:100]\n",
    "outfile=\"outfile/predictions%s.txt\"%(protein)\n",
    "fw = open(outfile, 'w')\n",
    "# myprob = \"\\n\".join(map(str, predictions[:, 0]))\n",
    "myprob = \"\\n\".join(map(str, test_y[:,0]))\n",
    "#fw.write(mylabel + '\\n')\n",
    "fw.write(myprob)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC:  0.6295193437500001\n",
      "0.8001 0.6295193437500001\n",
      "******************************************************\n",
      "# 7  - Protein:  17_ICLIP_HNRNPC_hg19\n",
      "******************************************************\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(test_y, predictions)\n",
    "print \"Test AUC: \", auc\n",
    "print model.evaluate(seq_test,test_y, verbose=0)[1], auc\n",
    "\n",
    "print \"******************************************************\"\n",
    "print \"#\", i ,\" - Protein: \", protein\n",
    "print \"******************************************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
